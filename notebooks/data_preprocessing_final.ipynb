{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f787cc-0714-4c46-b665-1e4cd0d1ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae5b79-e78b-4bf0-ab7c-208ae1534311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if code was already run, to save time\n",
    "clean_logs_df = pd.read_csv(\"data_month/clean_logs.csv\")\n",
    "unique_arks_df = pd.read_csv(\"data_month/unique_arks_df.csv\")\n",
    "arks_non_empty_df = pd.read_csv(\"arks_final_month/arks_non_empty.csv\")\n",
    "arks_empty_df = pd.read_csv(\"arks_final_month/arks_empty.csv\")\n",
    "enriched_logs_df = pd.read_csv(\"data_month/enriched_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ca522",
   "metadata": {},
   "source": [
    "## 1 : Extracting the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350eb895-70fa-4fca-b34c-2cb293f2e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logs for month of February\n",
    "folder_path = r'../../../lhstdata1/students/Gallica_logs/1LogGallicaFevrier2016'\n",
    "all_files = glob.glob(folder_path + \"/*.log.gz\")\n",
    "all_files.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5997453a-2dbd-4727-8725-b024cbda7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281f621-bb42-4269-8842-45e7ed1af565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all files:  27%|████▉             | 603/2201 [00:32<01:30, 17.75it/s]"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for f in tqdm(all_files, desc=\"Processing all files\"):\n",
    "    try:\n",
    "        # Read csv file into a Dask DataFrame\n",
    "        df = dd.read_csv(f, encoding='UTF-8', sep='\\t', header=None, blocksize=None)\n",
    "        # Append to list of Dask DataFrames\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {f}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99467a5-e45a-4b85-86ae-ddb66f63a230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_dfs = []\n",
    "for df in tqdm(dfs[:1100], desc=\"Computing DataFrames\"):\n",
    "    computed_dfs.append(df.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82286c91-8405-4770-aa01-19150d6ce5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in tqdm(dfs[1100:], desc=\"Computing DataFrames\"):\n",
    "    computed_dfs.append(df.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2b4b0-cf93-4461-b2ec-12c08a1d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(computed_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464241e8-0206-40e5-9ae6-96e52917e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebdacd-5b16-4533-ad2c-c5826f237ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"data_temp_month/raw_logs_month.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fdeaf",
   "metadata": {},
   "source": [
    "## 2 : Extracting features from logs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab612d7-f278-4e62-a1e6-9f7f91196c4d",
   "metadata": {},
   "source": [
    "Structure of a line : '##' then IP address, '##' then country (or null), '##' then city (or null), '##--' then date\n",
    "\n",
    "\n",
    "nothing, then HTTP request in quotes, with the protocol number\n",
    "followed by number of response (200 = OK), then size, then referrer website (or '-' if unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77476cda-2c85-4e16-a8d4-b383ef6300c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the lines to recover meaningful information - ip address, country, city, date and request\n",
    "lines_df = combined_df[0].str.split('##', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2dc7e-e644-4f22-a866-762a6c0eb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns with informative names\n",
    "lines_df = lines_df.rename(columns = {1:\"IPaddress\", 2:\"Country\", 3:\"City\", 4:\"Full_request\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0416eae-5864-4005-851f-405e0354874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150962bb-ec5b-4101-8905-6ca3febfb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dates\n",
    "temp = pd.DataFrame()\n",
    "temp['Date'] = lines_df.apply(lambda x: x['Full_request'].split(\"]\")[0].split(\"[\")[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c49a8e-79d4-4584-92c2-1412e949eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime format for sorting, to find earliest and oldest log\n",
    "temp_date = pd.DataFrame()\n",
    "temp_date['Date'] = pd.to_datetime(temp['Date'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "# sort by ascending order\n",
    "temp_date = temp_date.sort_values(by='Date', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfe03b-8997-4fc6-b719-8be0f96f8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date of first log:\", temp_date.iloc[0]['Date'])\n",
    "print(\"Date of last log:\", temp_date.iloc[-1]['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8c8d9-e952-4abf-8309-224845d87f7e",
   "metadata": {},
   "source": [
    "We have logs from 31/01/2016 - 13h to , a period of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47375d1-9747-4b59-8fd0-0114f0560f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract request\n",
    "temp['Request'] = lines_df.apply(lambda x: ' '.join(x['Full_request'].split(\"\\\"\")[1].split(' ')[:2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682218f-b1c5-4f74-a440-4b806d4b4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract referrer\n",
    "temp['Referrer'] = lines_df.apply(lambda x: x['Full_request'].split(\"\\\"\")[3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e95e9d-00da-416e-993b-a9bd3933cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract ark from the request\n",
    "def extract_ark(request):\n",
    "    # init ark variable\n",
    "    ark = '-'\n",
    "    # regular expression pattern, 12148 is specific to gallica\n",
    "    pattern = r'/12148/([^/.]+)'\n",
    "    # use regec to find ark in request string\n",
    "    match = re.search(pattern, request)\n",
    "    if match:\n",
    "        ark = match.group(1)\n",
    "    return ark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dbb8d-e31e-48b7-8a81-3ad08df91ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ark\n",
    "temp['Ark'] = temp.apply(lambda x: extract_ark(x['Request']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47ac1c-0222-441b-a518-b6ffd9fa903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the arks are not cut at the right place at their end\n",
    "# function to clean arks\n",
    "def clean_ark(ark):\n",
    "    # remove any query parameters\n",
    "    ark = re.sub(r'\\?.*$', '', ark)\n",
    "    # remove any trailing non-alphanumeric characters\n",
    "    ark = re.sub(r'[^a-zA-Z0-9]$', '', ark)\n",
    "    ark = re.sub(r'%20$', '', ark)\n",
    "    ark = re.sub(r',.*$', '', ark)\n",
    "    ark = re.sub(r';.*$', '', ark)\n",
    "    return ark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec9b10-77f2-42d5-b64e-64153d840c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning function to 'Ark' column\n",
    "temp.loc[:, 'Ark'] = temp['Ark'].apply(clean_ark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92ba86-a115-409a-a469-64221c131896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the information retrieved, drop full request column \n",
    "logs_df = pd.concat([lines_df, temp], axis=1)\n",
    "logs_df = logs_df.drop(['Full_request'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f63b33-8cf2-4f0f-9f0b-5f5658870056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract search terms from the request\n",
    "\n",
    "def extract_search_terms(request):\n",
    "    # parse URL\n",
    "    parsed_url = urlparse(request)\n",
    "    # extract query parameters\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    # extract search query from query parameters\n",
    "    search_query = query_params.get('query', [''])[0]\n",
    "    # URL-decode the search query\n",
    "    search_query = unquote(search_query)\n",
    "    # extract search terms using regular expression\n",
    "    search_terms = re.findall(r'\"([^\"]+)\"', search_query)\n",
    "    \n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb66659-efa2-4b73-86c7-187b436f5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract search terms\n",
    "logs_df['search_terms'] = logs_df.apply(lambda x: extract_search_terms(x['Request']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2d5a7-01ba-4899-ac13-b16edc0dca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9d756",
   "metadata": {},
   "source": [
    "## 3 : Enriching log data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b3949-b3b3-4e62-87f3-247bc1af9127",
   "metadata": {},
   "source": [
    "We want to enrich the data by adding additional information about the requested document (if there was one) : Dewey class, type of document, visibility. Among other things, this will help create a diversity metric for the sessions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7e015-6ae0-4393-8f76-34b1a28250b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique arks\n",
    "# replace unknown arks by None\n",
    "logs_df.loc[logs_df['Ark'] == '-', 'Ark'] = None\n",
    "# get non null arks\n",
    "arks_df = logs_df[logs_df['Ark'].notnull()].copy()\n",
    "# keep unique arks\n",
    "arks_df.drop_duplicates(subset=['Ark'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb0cec-2e3e-4d9f-a5cf-3f7e9f97ccfa",
   "metadata": {},
   "source": [
    "Some of the clean arks values are not arks. We re-clean with a function that specifies that arks must start with a 'b'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9840e-5949-4d33-9c3a-650dc3df0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_mask = arks_df['Ark'].str.startswith('b', na=False)\n",
    "arks_df = arks_df[b_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visibility metric for each ark\n",
    "# calculate the frequency of each ark\n",
    "arks_counts = logs_df['Ark'].value_counts()\n",
    "\n",
    "# calculate the visibility for each value in 'ark', if no ark visibility is 0\n",
    "logs_df['visibility'] = logs_df['Ark'].map(lambda x: arks_counts.get(x, 0) / len(logs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the clean logs\n",
    "logs_df.to_csv(\"data_month/clean_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9356a-79df-4185-8e61-190a40ad8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load arks that were already requested to remove them from the list of unique arks to request\n",
    "arks_already_requested = pd.read_csv(\"arks_final/processed_arks.csv\")\n",
    "arks_to_request = arks_df[~arks_df['Ark'].isin(arks_already_requested['Ark'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70b268-fdc2-48a4-abc9-9d4df8b88388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique arks\n",
    "arks_to_request['Ark'].to_csv(\"data_month/unique_arks_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff0236-a448-41c6-ac54-a828b3419408",
   "metadata": {},
   "source": [
    "The arks enable us to request the metadata of this document to Gallica. From that, we will extract the theme of the document and its Dewey class, if there is one. Only printed documents and prints have Dewey classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3d2f9-d857-46f9-956b-8ee559911ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAI request to Gallica\n",
    "def OAI(id):\n",
    "\n",
    "    OAI_BASEURL = 'https://gallica.bnf.fr/services/OAIRecord?ark='\n",
    "\n",
    "    url = \"\".join([OAI_BASEURL, id])\n",
    "\n",
    "    s = requests.get(url, stream=True)\n",
    "    soup = BeautifulSoup(s.content,\"lxml-xml\")\n",
    "    return soup\n",
    "\n",
    "# function to extract type and theme\n",
    "def extract_metadata(ark):\n",
    "    if ark == '-':\n",
    "        return ark, None, None\n",
    "    theme = ''\n",
    "    typedoc = ''\n",
    "    try:\n",
    "        oai_result = OAI(ark)\n",
    "        if oai_result is not None:\n",
    "            # extracting theme\n",
    "            sdewey_tag = oai_result.find(\"sdewey\")\n",
    "            if sdewey_tag:\n",
    "                theme = sdewey_tag.text\n",
    "        \n",
    "            # extracting typedoc\n",
    "            typedoc_tag = oai_result.find(\"typedoc\")\n",
    "            if typedoc_tag:\n",
    "                typedoc = typedoc_tag.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", e)\n",
    "\n",
    "    return ark, theme, typedoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b291d-efcb-40bf-9b29-ba6876b35efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to request the arks metadata, with a progress bar and saving batches to a folder\n",
    "def apply_with_progress(df, func, result_df, batch_size=20000, output_prefix='arks_temp_month/arks_batch'):\n",
    "    result = []\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for index, row in df.iterrows():\n",
    "            curr_result = func(row)\n",
    "            result.append(curr_result)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # save to CSV file every batch_size ARKs\n",
    "            if len(result) % batch_size == 0:\n",
    "                batch_df = pd.DataFrame(result, columns=['Ark', 'Theme', 'Type'])\n",
    "                batch_df.to_csv(f'{output_prefix}_{len(result)}.csv', index=False)\n",
    "    \n",
    "    return pd.DataFrame(result, columns=['Ark', 'Theme', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a8bf8-f993-4e4c-addc-72e29b5f5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes VERY long, do not run if arks' types and dewey classes are already computed\n",
    "result_df_arks = pd.DataFrame()\n",
    "all_arks = apply_with_progress(arks_to_request, lambda row: extract_metadata(row['Ark']), result_df_arks)\n",
    "\n",
    "# Save the remaining results to CSV\n",
    "all_arks.to_csv('arks_final_month/all_arks_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290cf56-5146-4673-b65d-2c37ed7f45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate empty and non empty arks\n",
    "non_empty_arks_df = all_arks[pd.notnull(arks_df['Type'])]\n",
    "# find empty typedoc and create new dataframe to request these arks\n",
    "empty_arks_df = all_arks[pd.isnull(arks_df['Type'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914794e-4591-4d12-b8ae-d6eb1177a526",
   "metadata": {},
   "source": [
    "Some arks have the structure btv1b90039### and when requested, yield \"Erreur d'utilisation.500\". We assume these refer to documents that are not available anymore on gallica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_empty_arks = len(empty_arks_df)\n",
    "print(\"There are\",nb_empty_arks , \"empty arks, which is\", nb_empty_arks/len(all_arks)*100, \"% of the arks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f0846-2570-49df-8d5b-907e8eb99240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save empty and non empty arks for now\n",
    "non_empty_arks_df.to_csv(\"arks_final_month/arks_non_empty.csv\")\n",
    "empty_arks_df.to_csv(\"arks_final_month/arks_empty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3514d22-1ae6-4d59-832e-0d502cd1a6e8",
   "metadata": {},
   "source": [
    "## 4 : Concatenating obtained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b594f73-a7ce-41e1-ad23-7c92f6f333e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop potential duplicates, save final complete arks\n",
    "arks_final_df = all_arks.drop_duplicates(subset=['Ark'])\n",
    "arks_final_df.to_csv(\"data_month/arks_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac56e9b-a6fe-455b-aed5-d678fa046885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DataFrames based on the 'Ark' column, keeping all rows from 'logs_df'\n",
    "logs_arks_df = pd.merge(logs_df, arks_final_df, on='Ark', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818be702-fe2c-4317-9cea-da8f4dc54f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_arks_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f06ca0-b904-49de-8d19-6e4d92d4c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "logs_arks_df = logs_arks_df.drop(['0', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6cc35-2aae-4768-a0d5-fe51fc926895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length is the same as the beggining\n",
    "len(logs_arks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c75818-c03a-4de8-9a41-081b30863771",
   "metadata": {},
   "source": [
    "#### Cleaning Nans in 'Theme' and 'Type' fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12d528-1fc0-4918-a00b-f12e5f9b6d24",
   "metadata": {},
   "source": [
    "The theme field can be Nan if either the document has no Dewey class, or if the document metadata was not retrieved. To differenciate these cases, we put Theme = 'No_dewey_class' when the document has no Dewey class, and 'Unknown' for Theme and Type when the document metadata was not available.\n",
    "The Type and Theme can also be Nan if the log has no ark (for example, when an action is done). In this case, we give it the value 'No_ark'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4155d9-7332-4558-b63a-f386917802dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_type_mask = logs_arks_df['Type'].notna() & logs_arks_df['Theme'].isna()\n",
    "# replacing 'Theme' values with -1 in rows where 'Theme' is Nan and 'Type' is known, indicating a document with no Dewey class\n",
    "logs_arks_df.loc[known_type_mask, 'Theme'] = 'No_dewey_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3455a4-766e-4974-b8c0-b5b90c909a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_ark_mask = logs_arks_df['Type'].isna() & logs_arks_df['Theme'].isna()\n",
    "# replacing 'Theme' and 'Type' values with 'Unknown, indicating the metadata was not available\n",
    "logs_arks_df.loc[unknown_ark_mask, ['Theme', 'Type']] = 'Unknown', 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa77e12-e1f5-4129-bbdd-249c4874be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ark_type_mask = logs_arks_df['Ark'].isna()\n",
    "# replacing 'Theme' and 'Type' values with 'no_ark' where 'Ark' is NaN\n",
    "logs_arks_df.loc[no_ark_type_mask, ['Theme', 'Type']] = 'No_ark', 'No_ark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f178eb-ac60-4890-a9ef-920745acdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_arks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be261f-e407-4d30-983a-f28f68ff5f6e",
   "metadata": {},
   "source": [
    "## 5 : Final step : saving the enriched logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e8c9d-5088-4490-b052-bbb5311137aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_arks_df.to_csv(\"data_month/enriched_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8eb35e-bfd1-470b-8ebe-5c5e4aa36c58",
   "metadata": {},
   "source": [
    "Our data is now pre-processed and enriched. The next step will be to create user sessions from it, and then classify part of these sessions as Rabbit Holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11654a-52c2-4848-93b2-7f7f724b00ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
